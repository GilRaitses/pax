I want to work on BRANCH 1: Vision Model Integration & Feature Extraction.

See agentHandoff/worktrees/MULTI_AGENT_WORK_TREE.md for full details.

Let's start with Task 1.1: Set up YOLOv8n for object detection.
- Install ultralytics package (add to requirements or pyproject.toml)
- Create wrapper script: src/pax/vision/__init__.py
- Create wrapper script: src/pax/vision/yolov8n.py
- Test on sample images from data/raw/images/ or GCS bucket
- Output should include: Pedestrian count, vehicle count, bike count
- Create basic tests: tests/test_yolov8n.py

Then Task 1.2: Set up Detectron2 for instance segmentation.
- Install detectron2 (check compatibility with Python version)
- Create wrapper: src/pax/vision/detectron2.py
- Configure for instance segmentation
- Test on sample images
- Output: Detailed object boundaries, crowd density metrics

Then Task 1.3: Set up CLIP for scene understanding.
- Install transformers or clip package
- Create wrapper: src/pax/vision/clip.py
- Create scene understanding wrapper
- Test on sample images
- Output: Scene labels, semantic features

Then Task 1.4: Create Feature Extraction Pipeline.
- Create unified feature extraction function: src/pax/vision/extractor.py
- Combine all three models (YOLOv8n, Detectron2, CLIP)
- Handle errors gracefully
- Create single script that extracts all features from an image
- Output: Single script that extracts all features

Finally Task 1.5: Extract Features from Current Images.
- Process all ~495 images currently collected
- Store features in structured format (JSON or Parquet)
- Generate extraction report
- Output: Feature dataset for all images

This work is foundational - other branches will use the features we extract here.
