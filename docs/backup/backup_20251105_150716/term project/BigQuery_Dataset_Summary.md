# BigQuery Dataset Summary & Local Model Training Guide
## NYC Vibe Check - Term Project Data Analysis

**Analysis Date:** October 27, 2025  
**Project:** CIS 667 Term Project - Adaptive Heuristic Learning for Pedestrian Navigation  
**Student:** Gil Raitses

---

## Executive Summary

Your Google Cloud account is **ACTIVE** and contains a working BigQuery infrastructure with Vision AI-encoded data. You have **3 datasets** with **11 tables** containing **5 records** of structured ML training data. The data is **export-ready** and suitable for local model training.

### âœ… Key Findings

- **Data Available**: 5 total records across 3 tables (zone_analyses, violations, predictions)
- **Feature Engineering**: 17-dimensional numerical vectors ready for ML
- **Ground Truth**: Google Cloud Vision API labels for validation
- **Export Status**: Successfully downloaded to local machine
- **Quality**: Well-structured schema with clear field definitions

### âš ï¸ Limitations

- **Volume**: Limited sample size (5 records) requires synthetic data augmentation
- **Coverage**: Only 3 unique cameras represented
- **Temporal**: Single-day snapshot (June 26, 2025)

---

## Dataset Inventory

### 1. **zone_analyses** Table (2 records)

**Purpose**: Camera zone analysis with Vision AI encoding

**Key Features**:
- **17-dimensional numerical feature vectors**
  - Example: `[2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0]`
  - Represents: pedestrian walkway violations, bike positioning, red light violations, density, infrastructure quality, etc.
  
- **Temperature/Stress Scores**
  - Range: 5.0 - 24.0
  - Mean: 14.5
  - Purpose: Proxy metric for route stress/safety
  
- **Google Cloud Vision API Results**
  - Object detection: pedestrians, bicycles, vehicles
  - Scene labels: 17 labels per successful analysis
  - Top labels: "Road" (0.947), "Highway" (0.907), "Night" (0.858)
  - Safety scores: 0-10 scale
  
- **Processing Telemetry**
  - Success rates: 100% camera lookup, 50% image fetch, 50% vision analysis
  - Processing time: 717ms - 10,062ms (mean: 5,390ms)
  - Image sizes: ~18KB

**Sample Record Structure**:
```json
{
  "camera_id": "BK_011",
  "zone_id": "BK_011",
  "temperature_score": 24.0,
  "numerical_data": [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0.3, 0],
  "cloud_vision_data": {
    "pedestrian_count": 0,
    "bicycle_count": 0,
    "vehicle_count": 0,
    "safety_score": 4.7,
    "detected_labels": [...]
  },
  "timestamp": "2025-06-26 06:51:04"
}
```

### 2. **realtime_violations** Table (1 record)

**Purpose**: Detected safety violations with confidence scores

**Key Features**:
- **Violation Types**: bike_red_light_violation
- **Confidence Scores**: 0.87 (87% confidence)
- **Camera Association**: UUID-based tracking
- **Timestamps**: Precise violation detection times

**Sample Record**:
```json
{
  "event_id": "test-violation-1750922793988",
  "camera_id": "83404149-7deb-43ee-81b5-66fe804c0feb",
  "violation_type": "bike_red_light_violation",
  "confidence": 0.87,
  "timestamp": "2025-06-26 07:26:33"
}
```

### 3. **traffic_predictions** Table (2 records)

**Purpose**: ARIMA-based traffic forecasting

**Key Features**:
- **Model Versions**: arima_plus_v1.0, arima_plus_simulation_v1.0
- **Prediction Accuracy (MAPE)**:
  - Range: 0.12 - 0.17 (12-17% error)
  - Mean: 0.146 (14.6% error)
  - Lower MAPE = better accuracy
  
- **Predicted Values**:
  - Vehicles: 4-15 (mean: 9.5)
  - Pedestrians: 4-8 (mean: 6.0)
  
- **Prediction Horizon**: 30 minutes ahead

**Sample Record**:
```json
{
  "prediction_id": "test-prediction-1750922795333",
  "camera_id": "83404149-7deb-43ee-81b5-66fe804c0feb",
  "predicted_vehicles": 15,
  "predicted_pedestrians": 8,
  "horizon_minutes": 30,
  "mape": 0.12,
  "model_version": "arima_plus_v1.0",
  "timestamp": "2025-06-26 07:26:35"
}
```

---

## Feature Vector Interpretation

### 17-Dimensional Numerical Data Array

Based on your vibe-check codebase, the 17 features likely represent:

| Index | Feature Name | Description | Range |
|-------|-------------|-------------|-------|
| 0 | pedestrian_walkway_violation | Bikes on pedestrian walkways | 0-3 |
| 1 | dangerous_bike_lane_position | Poor bike lane positioning | 0-3 |
| 2 | bike_red_light_violation | Bikes running red lights | 0-3 |
| 3 | blocking_pedestrian_flow | Flow obstruction | 0-3 |
| 4 | car_bike_lane_violation | Cars in bike lanes | 0-3 |
| 5 | pedestrian_density | Crowd density | 0-3 |
| 6 | vulnerable_population | Elderly, children, disabled | 0-3 |
| 7 | traffic_volume | Vehicle count | 0-3 |
| 8 | visibility_conditions | Lighting, weather | 0-2 |
| 9 | missing_barriers | Infrastructure gaps | 0-2 |
| 10 | poor_signage | Sign quality | 0-2 |
| 11 | signal_malfunction | Traffic signal issues | 0-2 |
| 12 | cyclist_speed_estimate | Speed assessment | 0-3 |
| 13 | aggressive_behavior | Aggressive actions | 0-2 |
| 14 | infrastructure_quality | Overall quality | 0-2 |
| 15 | weather_impact | Weather effects | 0.0-1.0 |
| 16 | overall_safety_risk | Aggregate risk | 0-3 |

**Interpretation**:
- **0** = None/absent
- **1** = Low/minor
- **2** = Moderate
- **3** = High/severe

---

## Data Quality Assessment

### âœ… Strengths

1. **Well-Structured Schema**
   - Clear field definitions
   - Consistent data types
   - Proper timestamps

2. **Feature Engineering Complete**
   - 17-dimensional vectors ready for ML
   - Normalized scales (0-3 range)
   - Interpretable features

3. **Ground Truth Available**
   - Cloud Vision API provides validation
   - Confidence scores for verification
   - Multiple data sources (Vision + manual encoding)

4. **Processing Transparency**
   - Pipeline status tracking
   - Performance metrics
   - Error handling documentation

### âš ï¸ Limitations

1. **Sample Size**
   - Only 5 records total
   - Insufficient for statistical significance
   - **Mitigation**: Synthetic data generation required

2. **Temporal Coverage**
   - Single day snapshot (June 26, 2025)
   - No longitudinal trends
   - **Mitigation**: Can collect more or use bootstrapping

3. **Geographic Coverage**
   - 3 unique cameras only
   - Limited to 2 boroughs (Brooklyn, Bronx)
   - **Mitigation**: Augment with synthetic geographic variation

4. **Pipeline Issues**
   - 50% image fetch failure rate
   - 50% vision analysis failure
   - **Mitigation**: Use successfully processed records as templates

---

## Suitability for Term Project

### âœ… Viable Applications

1. **Heuristic Learning Input**
   - 17-dimensional features â†’ base heuristic components
   - Temperature score â†’ target variable for stress optimization
   - Can learn weighted combinations from existing features

2. **Baseline Model Training**
   - ARIMA predictions provide baseline accuracy (14.6% MAPE)
   - Your adaptive heuristics should improve upon this
   - Clear success metric: beat 14.6% error

3. **Feature Importance Analysis**
   - Identify which of 17 features most predict stress
   - Use for heuristic weight initialization
   - Interpretable for term project write-up

4. **Validation Framework**
   - Vision API labels as ground truth
   - Temperature scores as target
   - Confidence scores for weighting samples

### ðŸŽ¯ Recommended Approach

Given limited sample size, use a **hybrid strategy**:

1. **Real Data**: Use 5 records as validation/test set
2. **Synthetic Data**: Generate training set using:
   - **SMOTE** (Synthetic Minority Over-sampling)
   - **Bootstrap resampling** with noise injection
   - **Parametric generation** based on feature distributions
3. **Transfer Learning**: Initialize weights from domain knowledge
4. **Cross-Validation**: K-fold on synthetic + real combined

---

## Local Model Training Guide

### Step 1: Environment Setup

```bash
cd "/Users/gilraitses/cis667/term project"
python3 -m venv venv
source venv/bin/activate
pip install pandas numpy scikit-learn matplotlib seaborn jupyter
```

### Step 2: Load and Parse Data

```python
import json
import pandas as pd
import numpy as np

# Load zone analyses
with open('bigquery_data/zone_analyses.json', 'r') as f:
    zone_data = json.load(f)

# Parse feature vectors
features = []
targets = []

for record in zone_data:
    analysis = json.loads(record['analysis_json'])
    if 'numerical_data' in analysis:
        features.append(analysis['numerical_data'])
        targets.append(float(record['temperature_score']))

X = np.array(features)  # Shape: (n_samples, 17)
y = np.array(targets)   # Shape: (n_samples,)
```

### Step 3: Generate Synthetic Training Data

```python
from sklearn.utils import resample
from imblearn.over_sampling import SMOTE

# Bootstrap resampling (replicate with noise)
def bootstrap_with_noise(X, y, n_samples=100, noise_std=0.1):
    X_boot, y_boot = resample(X, y, n_samples=n_samples, replace=True)
    # Add Gaussian noise
    noise = np.random.normal(0, noise_std, X_boot.shape)
    X_boot = np.clip(X_boot + noise, 0, 3)  # Keep in valid range
    return X_boot, y_boot

# Generate 100 synthetic samples
X_synthetic, y_synthetic = bootstrap_with_noise(X, y, n_samples=100)

# Combine real and synthetic
X_train = np.vstack([X, X_synthetic])
y_train = np.concatenate([y, y_synthetic])
```

### Step 4: Train Baseline Models

```python
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

# Linear weighted combination (simple heuristic learning)
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)

print("Learned Heuristic Weights:")
for i, weight in enumerate(ridge_model.coef_):
    print(f"  Feature {i+1:2d}: {weight:+.4f}")

# Evaluate on real data (leave-one-out)
from sklearn.model_selection import LeaveOneOut
loo = LeaveOneOut()
scores = cross_val_score(ridge_model, X, y, cv=loo, scoring='neg_mean_absolute_error')
print(f"\nMean Absolute Error (real data): {-scores.mean():.2f}")
```

### Step 5: Compare to Weighted A* Approach

```python
# Implement weighted heuristic function
def learned_heuristic(node_features, weights):
    """
    Adaptive heuristic using learned weights
    
    Args:
        node_features: 17-dim feature vector for a route node
        weights: learned weights from Ridge regression
    
    Returns:
        Heuristic value (predicted stress score)
    """
    return np.dot(node_features, weights)

# Test different weight parameters (like Programming Problem 3)
W_values = [1.0, 1.2, 1.5]
for W in W_values:
    weighted_coef = ridge_model.coef_ * W
    predictions = X @ weighted_coef + ridge_model.intercept_
    mae = np.mean(np.abs(predictions - y))
    print(f"W={W}: MAE={mae:.2f}")
```

### Step 6: Export Model for NYC Vibe Check Integration

```python
import pickle

# Save trained model
model_data = {
    'weights': ridge_model.coef_,
    'intercept': ridge_model.intercept_,
    'feature_names': [f'feature_{i}' for i in range(17)],
    'training_date': '2025-10-27',
    'mae': -scores.mean()
}

with open('learned_heuristic_model.pkl', 'wb') as f:
    pickle.dump(model_data, f)

print("\nâœ“ Model saved to: learned_heuristic_model.pkl")
print("âœ“ Ready for integration with pedestrianRouteService.ts")
```

---

## Integration with NYC Vibe Check

### Update TypeScript Route Service

```typescript
// Add to pedestrianRouteService.ts

interface LearnedHeuristicModel {
  weights: number[];
  intercept: number;
  mae: number;
}

class AdaptiveHeuristicRouter {
  private learnedWeights: number[];
  
  constructor() {
    // Load learned weights from Python model
    this.learnedWeights = this.loadLearnedWeights();
  }
  
  private loadLearnedWeights(): number[] {
    // TODO: Load from Firebase or local JSON
    // For now, use placeholder
    return Array(17).fill(0.1);
  }
  
  private computeLearnedHeuristic(zoneFeatures: number[]): number {
    // Dot product: weights Â· features
    return this.learnedWeights.reduce(
      (sum, w, i) => sum + w * zoneFeatures[i],
      0
    );
  }
  
  async findOptimalRoute(
    start: string, 
    goal: string, 
    adaptiveWeight: number = 1.2
  ): Promise<Route> {
    // Use learned heuristic in A* search
    const heuristic = (node: Node) => {
      const features = this.extractFeatures(node);
      const h_learned = this.computeLearnedHeuristic(features);
      return adaptiveWeight * h_learned;  // Weighted heuristic
    };
    
    return this.astar(start, goal, heuristic);
  }
}
```

---

## Next Steps for Term Project

### Immediate (Week 1-2)

1. **âœ“ Data Export Complete** - 5 records exported and analyzed
2. **â†’ Synthetic Data Generation** - Create 100-500 training samples
3. **â†’ Baseline Model Training** - Ridge regression on combined dataset
4. **â†’ Feature Importance Analysis** - Identify top 5 predictive features

### Short-term (Week 3-4)

5. **â†’ Implement Adaptive Regularization** - W âˆˆ [1.0, 2.0] like Programming Problem 3
6. **â†’ Path Sampling Framework** - Monte Carlo sampling for route diversity
7. **â†’ Integration Testing** - Connect Python model to TypeScript service
8. **â†’ Validation Experiments** - Compare to standard A* baseline

### Long-term (Week 5-10)

9. **â†’ Collect Additional Data** - Activate NYC Vibe Check to gather more samples
10. **â†’ Neural Network Heuristic** - If time permits, implement deep learning version
11. **â†’ Full System Deployment** - Deploy to Firebase for live testing
12. **â†’ Final Report & Analysis** - Write up results with statistical significance

---

## Cost Considerations

### Current Status: âœ… FREE TIER

- **BigQuery Storage**: <1GB (well within free 10GB/month)
- **BigQuery Queries**: 5 queries total (~1MB processed, free 1TB/month)
- **Cloud Functions**: Deleted expensive functions (July 22, 2025)
- **Monthly Cost**: $0 (no charges expected)

### To Avoid Charges

1. **Don't restart deleted Cloud Functions** (ML processing, analytics)
2. **Keep BigQuery queries minimal** (<1TB/month is free)
3. **Export data locally before extensive experimentation**
4. **Use local Python/sklearn instead of BigQuery ML for training**

---

## Summary

### What You Have

âœ… **Active Google Cloud account** with BigQuery access  
âœ… **3 datasets, 11 tables** with proper schema  
âœ… **5 training records** with 17-dimensional features  
âœ… **Vision AI integration** for ground truth validation  
âœ… **Export capability** - data downloaded locally  
âœ… **Cost control** - all expensive services disabled

### What You Need

âš ï¸ **More training data** (synthetic generation or collection)  
âš ï¸ **Baseline comparisons** (implement standard A*, Dijkstra)  
âš ï¸ **Validation framework** (cross-validation, statistical tests)  
âš ï¸ **Integration code** (Python â†” TypeScript bridge)

### Feasibility Assessment

**âœ“ TERM PROJECT IS VIABLE**

Your existing data provides:
- Real-world feature vectors for heuristic learning
- Ground truth labels for validation
- Baseline accuracy metrics (14.6% MAPE) to beat
- Integration points with working NYC Vibe Check system

With synthetic data augmentation and proper experimental design, you have everything needed for a successful CIS 667 term project demonstrating adaptive heuristic learning for stress-optimized pedestrian navigation.

---

## Files Created

1. `/Users/gilraitses/cis667/term project/bigquery_data/zone_analyses.json` (4.0KB)
2. `/Users/gilraitses/cis667/term project/bigquery_data/realtime_violations.json` (288B)
3. `/Users/gilraitses/cis667/term project/bigquery_data/traffic_predictions.json` (529B)
4. `/Users/gilraitses/cis667/term project/bigquery_data/eda_analysis.py` (Python EDA script)
5. `/Users/gilraitses/cis667/term project/BigQuery_Dataset_Summary.md` (this document)

---

**Analysis Complete**  
**Date:** October 27, 2025  
**Status:** Ready for local model training and term project development

