<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Scale Signal Processing for Learned Heuristic Pathfinding</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        :root {
            /* Cinnamoroll Color Palette */
            --cinnamoroll-blue: #8EC8EA;
            --cinnamoroll-pink: #FFB6C1;
            --cinnamoroll-cream: #FFFAF0;
            --cinnamoroll-lavender: #E6E6FA;
            --cinnamoroll-mint: #BDFCC9;
            --cinnamoroll-periwinkle: #CCCCFF;
            --cinnamoroll-deep-blue: #6495ED;
            --cinnamoroll-soft-gray: #F0F0F5;
            --header-purple: #4B0082;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Avenir Next', 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, var(--cinnamoroll-lavender) 0%, var(--cinnamoroll-periwinkle) 100%);
            color: #333;
            padding: 20px;
            min-height: 100vh;
            line-height: 1.6;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 16px;
            box-shadow: 0 8px 24px rgba(0, 0, 0, 0.1);
            padding: 40px;
        }
        
        header {
            text-align: center;
            margin-bottom: 50px;
            padding-bottom: 30px;
            border-bottom: 3px solid var(--cinnamoroll-deep-blue);
        }
        
        h1 {
            font-size: 2.8em;
            font-weight: 700;
            margin-bottom: 15px;
            color: var(--cinnamoroll-deep-blue);
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        .subtitle {
            font-size: 1.3em;
            color: var(--cinnamoroll-blue);
            margin-bottom: 10px;
            font-weight: 500;
        }
        
        .project-info {
            margin-top: 20px;
            font-size: 0.95em;
            color: #666;
        }
        
        .cover-figures {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }
        
        .figure-card {
            background: var(--cinnamoroll-cream);
            padding: 25px;
            border-radius: 12px;
            border-left: 5px solid var(--cinnamoroll-deep-blue);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        
        .figure-card h3 {
            color: var(--cinnamoroll-deep-blue);
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.4em;
        }
        
        .figure-card img {
            width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15);
            margin-bottom: 15px;
        }
        
        .figure-caption {
            font-size: 0.95em;
            line-height: 1.7;
            color: #555;
            font-style: italic;
        }
        
        .section {
            margin: 50px 0;
        }
        
        h2 {
            color: var(--cinnamoroll-deep-blue);
            font-size: 2.2em;
            margin-bottom: 25px;
            border-bottom: 2px solid var(--cinnamoroll-blue);
            padding-bottom: 10px;
        }
        
        h3 {
            color: var(--cinnamoroll-blue);
            font-size: 1.5em;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        
        .problem-section {
            background: var(--cinnamoroll-lavender);
            padding: 30px;
            border-radius: 12px;
            border-left: 5px solid var(--cinnamoroll-pink);
            margin: 30px 0;
        }
        
        .comic-container {
            text-align: center;
            margin: 30px 0;
        }
        
        .comic-container img {
            max-width: 400px;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        .comic-caption {
            margin-top: 15px;
            font-style: italic;
            color: #666;
            font-size: 0.9em;
        }
        
        .info-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .info-card {
            background: var(--cinnamoroll-soft-gray);
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid var(--cinnamoroll-blue);
        }
        
        .info-card h4 {
            color: var(--cinnamoroll-pink);
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.2em;
        }
        
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .stat-card {
            background: white;
            border-radius: 12px;
            padding: 25px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            border-left: 4px solid var(--cinnamoroll-deep-blue);
            transition: transform 0.2s;
        }
        
        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 12px rgba(0, 0, 0, 0.15);
        }
        
        .stat-label {
            font-size: 0.9em;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 8px;
        }
        
        .stat-value {
            font-size: 2.5em;
            font-weight: 700;
            color: var(--cinnamoroll-deep-blue);
        }
        
        .stat-subtext {
            font-size: 0.85em;
            color: #999;
            margin-top: 5px;
        }
        
        .image-viewer {
            background: var(--cinnamoroll-cream);
            padding: 25px;
            border-radius: 12px;
            border-left: 5px solid var(--cinnamoroll-deep-blue);
            margin: 30px 0;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        
        .image-viewer h3 {
            margin-top: 0;
            color: var(--cinnamoroll-deep-blue);
        }
        
        .image-viewer-container {
            text-align: center;
            margin: 20px 0;
        }
        
        .image-viewer-container img {
            max-width: 100%;
            max-height: 600px;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
            border: 3px solid var(--cinnamoroll-blue);
        }
        
        .image-viewer-meta {
            margin-top: 15px;
            font-size: 0.9em;
            color: #666;
        }
        
        .camera-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        
        .camera-card {
            background: white;
            border-radius: 12px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            border-left: 3px solid var(--cinnamoroll-mint);
        }
        
        .camera-header {
            display: flex;
            justify-content: space-between;
            align-items: start;
            margin-bottom: 12px;
        }
        
        .camera-name {
            font-weight: 600;
            font-size: 1.05em;
            color: #333;
            flex: 1;
        }
        
        .camera-badge {
            background: var(--cinnamoroll-deep-blue);
            color: white;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.75em;
            font-weight: 600;
            text-transform: uppercase;
        }
        
        .camera-meta {
            font-size: 0.85em;
            color: #666;
            margin-bottom: 8px;
        }
        
        .camera-count {
            font-size: 1.8em;
            font-weight: 700;
            color: var(--cinnamoroll-deep-blue);
            margin: 10px 0;
        }
        
        .camera-last-capture {
            font-size: 0.8em;
            color: #999;
        }
        
        .refresh-indicator {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background: white;
            padding: 12px 20px;
            border-radius: 25px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            font-size: 0.9em;
            display: flex;
            align-items: center;
            gap: 10px;
            z-index: 1000;
        }
        
        .spinner {
            width: 16px;
            height: 16px;
            border: 2px solid #f3f3f3;
            border-top: 2px solid var(--cinnamoroll-deep-blue);
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }
        
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        
        .error-message {
            background: #fed7d7;
            color: #c53030;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 20px;
            display: none;
        }
        
        code {
            background: var(--cinnamoroll-soft-gray);
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: var(--cinnamoroll-deep-blue);
        }
        
        .methodology-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        
        .methodology-item {
            background: var(--cinnamoroll-lavender);
            padding: 15px;
            border-radius: 8px;
            border-left: 3px solid var(--cinnamoroll-deep-blue);
        }
        
        .methodology-item strong {
            color: var(--cinnamoroll-deep-blue);
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Multi-Scale Signal Processing for Learned Heuristic Pathfinding</h1>
            <div class="subtitle">Manhattan Navigation via Dynamic System Modeling</div>
            <div class="project-info">
                <div>CIS 667: Introduction to Artificial Intelligence – Term Project</div>
                <div style="margin-top: 8px;">Gil Raitses | Syracuse University | Data Science / HCAI Program</div>
            </div>
        </header>

        <!-- Cover Figures -->
        <div class="cover-figures">
            <div class="figure-card">
                <h3>Problem Space</h3>
                <img src="docs/figures/problem_space.png" 
                     alt="Problem Space: Multi-Scale Coverage Zones" 
                     onerror="this.src='docs/figures/problem_space.png'; this.onerror=function(){this.src='scripts/2025-11-09/outputs/problem_space.png';};">
                <div class="figure-caption">
                    <strong>Problem Space for Undirected Graph Search with Multi-Scale Coverage Zones.</strong> 
                    Spatial coverage zones (defined by traffic camera locations) partition the search space to enable 
                    multi-scale resolution analysis. The coverage zones represent different levels of granularity needed 
                    to determine Pareto-optimal solutions, allowing the search algorithm to efficiently explore the solution 
                    frontier by considering both local neighborhood characteristics and global path structure. The red boundary 
                    defines the problem space between Grand Central Station (START) and Carnegie Hall (GOAL), with coverage zones 
                    providing the multi-scale resolution necessary for evaluating trade-offs in the Pareto front.
                </div>
            </div>
            
            <div class="figure-card">
                <h3>Camera Corridor</h3>
                <img src="docs/figures/camera_corridor_partition.png" 
                     alt="Camera Corridor with Extended Coverage Zones"
                     onerror="this.src='docs/figures/camera_corridor_partition.png'; this.onerror=function(){this.src='scripts/2025-11-09/outputs/camera_corridor_partition.png';};">
                <div class="figure-caption">
                    <strong>Camera Corridor with Extended Coverage Zones.</strong> 
                    Extended camera corridor (purple boundary: 34th-66th St, 3rd-9th/Amsterdam) containing 82 traffic cameras 
                    used for multi-scale resolution analysis. The problem space (red boundary: 40th-61st St, Lex-8th/CPW) is nested 
                    within this wider corridor, demonstrating the relationship between the constrained search space and the extended 
                    coverage area needed for Pareto front determination. Each camera defines a coverage zone through Voronoi tessellation, 
                    with numbered markers indicating cameras within the purple corridor.
                </div>
            </div>
        </div>

        <!-- Introduction -->
        <div class="section">
            <h2>Introduction</h2>
            
            <h3>Subject Area</h3>
            <p>The work operates within <strong>Artificial Intelligence Search and Reasoning Systems</strong>, focusing on heuristic search, 
            knowledge representation and perception-based reasoning. It links foundational AI theory to sensory modeling using computer 
            vision and urban data streams.</p>
            
            <p>Classical pathfinding algorithms (A*, weighted A*) are extended by incorporating perceptually informed heuristics derived 
            from live camera data. This connects abstract state-space search to embodied decision-making in dynamic environments.</p>
            
            <h3>Project Type and Rationale</h3>
            <p>An <strong>Applied Project</strong> applies known methods to realistic situations, blending heuristic search (A*, weighted A*) 
            with machine learning (Ridge regression, feature engineering) on real urban data to evaluate perceptually informed navigation.</p>
            
            <p>The work applies course concepts (uninformed search, informed search, constraint satisfaction, knowledge representation) to 
            sensory data from a dynamic environment. Cost and optimality are extended to include perceptual load and behavioral friction.</p>
        </div>

        <!-- Problem Statement -->
        <div class="section">
            <h2>Problem Statement</h2>
            
            <div class="problem-section">
                <p>Traditional pathfinding algorithms optimize for geometric distance or travel time but fail to capture the 
                <strong>energetic and perceptual dynamics</strong> of movement in dense urban environments. In Manhattan, the cost 
                of traversing a route is not defined by distance alone but by <strong>sensory and behavioral complexity</strong>: 
                crowding, visual noise, infrastructure gaps and movement unpredictability.</p>
                
                <div class="comic-container">
                    <img src="docs/assets/carnegie-hall-comic.png" 
                         alt="Carnegie Hall comic" 
                         onerror="this.src='scripts/2025-11-10/carnegie-hall-comic.png'; this.onerror=function(){this.src='docs/backup_20251105_150716/term project/carnegie-hall-comic.png';};">
                    <div class="comic-caption">
                        Carnegie Hall parable: the project asks how an agent can learn to navigate from Grand Central to Carnegie Hall 
                        while accounting for perceptual effort instead of pure geometric distance.
                    </div>
                </div>
                
                <p>This work develops a <strong>perceptually informed heuristic</strong> for A* search using NYC traffic camera data. 
                The heuristic models environmental resistance by extracting features from visual data. Human pedestrians naturally avoid 
                stressful routes based on crowded intersections, poor lighting and aggressive traffic patterns; this approach encodes 
                similar avoidance behavior.</p>
                
                <p><strong>Key Research Question:</strong> Can learned heuristics from vision-based environmental features outperform 
                standard Manhattan-distance baselines in predicting route stress and improving path interpretability?</p>
            </div>
        </div>

        <!-- Approach -->
        <div class="section">
            <h2>Approach</h2>
            
            <div class="info-grid">
                <div class="info-card">
                    <h4>Problem</h4>
                    <p>Traditional pathfinding algorithms optimize for geometric distance but fail to capture the 
                    <strong>energetic and perceptual dynamics</strong> of movement in dense urban environments. 
                    Routes involve sensory complexity: crowding, visual noise, infrastructure gaps, and unpredictability.</p>
                </div>
                <div class="info-card">
                    <h4>Approach</h4>
                    <p>Develop a <strong>perceptually informed heuristic</strong> for A* search using NYC traffic camera data. 
                    The heuristic models environmental resistance by extracting features from visual data—encoding 
                    how pedestrians naturally avoid stressful routes.</p>
                </div>
                <div class="info-card">
                    <h4>Study Area</h4>
                    <p><strong>Grand Central → Carnegie Hall corridor</strong><br>
                    Manhattan: 40th-61st Streets, Lexington Ave to 8th Ave<br>
                    161 intersection nodes • 82 camera zones<br>
                    Start: Grand Central (42nd & Park)<br>
                    Goal: Carnegie Hall (57th & 7th Ave)</p>
                </div>
            </div>
        </div>

        <!-- Methodology -->
        <div class="section">
            <h2>Methodology</h2>
            
            <div class="methodology-grid">
                <div class="methodology-item">
                    <strong>State Space:</strong> 161 intersections from NYC DCM street centerlines
                </div>
                <div class="methodology-item">
                    <strong>Sensors:</strong> 82 cameras providing multi-dimensional feature vectors
                </div>
                <div class="methodology-item">
                    <strong>Heuristic:</strong> Learned via Ridge regression: <code>h(n) = w^T * f(zone(n))</code>
                </div>
                <div class="methodology-item">
                    <strong>Search:</strong> A* with Manhattan baseline vs. learned perceptual heuristic
                </div>
            </div>
        </div>

        <!-- Data Collection Dashboard -->
        <div class="section">
            <h2>Data Collection Dashboard</h2>
            
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-label">Total Images Collected</div>
                    <div class="stat-value" id="totalImages">0</div>
                    <div class="stat-subtext" id="totalImagesSubtext">From GCS bucket</div>
                </div>
                
                <div class="stat-card">
                    <div class="stat-label">Purple Zone Cameras</div>
                    <div class="stat-value" id="activeCameras">82</div>
                    <div class="stat-subtext">34th-66th St, 3rd-9th/Amsterdam</div>
                </div>
                
                <div class="stat-card">
                    <div class="stat-label">Today's Images</div>
                    <div class="stat-value" id="todayImages">0</div>
                    <div class="stat-subtext" id="todayImagesSubtext">Last 24 hours</div>
                </div>
                
                <div class="stat-card">
                    <div class="stat-label">Latest Capture</div>
                    <div class="stat-value" id="latestCapture" style="font-size: 1.2em;">--</div>
                    <div class="stat-subtext" id="latestCaptureSubtext">Eastern time</div>
                </div>
                
                <div class="stat-card">
                    <div class="stat-label">Collection Period</div>
                    <div class="stat-value" id="collectionDays" style="font-size: 1.5em;">0</div>
                    <div class="stat-subtext" id="collectionPeriod">Days active</div>
                </div>
                
                <div class="stat-card">
                    <div class="stat-label">Collection Rate</div>
                    <div class="stat-value" id="collectionRate" style="font-size: 1.5em;">48</div>
                    <div class="stat-subtext">Images/camera/day (30 min intervals)</div>
                </div>
            </div>

            <!-- Latest Image Viewer -->
            <div class="image-viewer">
                <h3>Most Recently Collected Image</h3>
                <div class="image-viewer-container" id="latestImageViewer">
                    <div style="padding: 40px; color: #999; font-style: italic;">
                        Loading latest image...
                    </div>
                </div>
                <div class="image-viewer-meta" id="latestImageMeta">
                    Camera: <span id="latestImageCamera">--</span><br>
                    Captured: <span id="latestImageTime">--</span>
                </div>
            </div>
        </div>

        <!-- Collection Status -->
        <div class="section">
            <h2>Collection Status</h2>
            
            <div class="info-card" style="margin-bottom: 20px;">
                <h4>Current Collection</h4>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-top: 15px;">
                    <div>
                        <strong>Cameras:</strong> 82 (numbered 1-82)<br>
                        <strong>Zone:</strong> Purple corridor (34th-66th St)
                    </div>
                    <div>
                        <strong>Expected:</strong> 3,936 images/day<br>
                        <strong>Target:</strong> 672 images/camera over 14 days
                    </div>
                    <div>
                        <strong>Job:</strong> pax-collector<br>
                        <strong>Scheduler:</strong> pax-collector-schedule (every 30 minutes)
                    </div>
                </div>
            </div>
            
            <div class="camera-grid" id="cameraGrid">
                <!-- Camera cards will be inserted here -->
            </div>
        </div>
        
        <div class="error-message" id="errorMessage"></div>
        
        <div class="refresh-indicator">
            <div class="spinner"></div>
            <span>Auto-refresh every 30s</span>
        </div>
    </div>
    
    <script>
        // Try multiple paths for stats.json (GitHub Pages compatibility)
        const STATS_PATHS = [
            './stats.json',
            './docs/stats.json',
            'stats.json',
            'docs/stats.json'
        ];
        
        const API_BASE = window.location.hostname === 'localhost' 
            ? 'http://localhost:8000' 
            : '';
        
        let cameraManifest = [];
        let statsData = null;
        let latestImageUrl = null;
        
        async function loadManifest() {
            try {
                let manifestPath = './data/manifests/corridor_cameras_numbered.json';
                let response = await fetch(manifestPath);
                
                if (!response.ok) {
                    if (API_BASE) {
                        response = await fetch(`${API_BASE}/api/manifest`);
                    } else {
                        manifestPath = './cameras.yaml';
                        response = await fetch(manifestPath);
                    }
                }
                
                if (response.ok) {
                    const manifest = await response.json();
                    cameraManifest = manifest.cameras || [];
                    console.log(`Loaded ${cameraManifest.length} cameras from manifest`);
                }
            } catch (error) {
                console.warn('Failed to load manifest:', error);
            }
        }
        
        async function fetchLatestImage(stats) {
            if (!stats.latestCapture || !stats.latestCameraId) {
                document.getElementById('latestImageViewer').innerHTML = 
                    '<div style="padding: 40px; color: #999; font-style: italic;">No images collected yet</div>';
                return;
            }
            
            try {
                // Try to construct GCS public URL or use API
                const imageUrl = stats.latestImageUrl || 
                    `https://storage.googleapis.com/pax-nyc-images/images/${stats.latestImagePath}` ||
                    `${API_BASE}/api/images/latest`;
                
                const viewer = document.getElementById('latestImageViewer');
                viewer.innerHTML = `<img src="${imageUrl}" alt="Latest collected image" onerror="this.parentElement.innerHTML='<div style=\\'padding: 40px; color: #999; font-style: italic;\\'>Image unavailable</div>';">`;
                
                document.getElementById('latestImageCamera').textContent = stats.latestCameraName || stats.latestCameraId;
                document.getElementById('latestImageTime').textContent = new Date(stats.latestCapture).toLocaleString();
                
                latestImageUrl = imageUrl;
            } catch (error) {
                console.warn('Failed to load latest image:', error);
                document.getElementById('latestImageViewer').innerHTML = 
                    '<div style="padding: 40px; color: #999; font-style: italic;">Image unavailable</div>';
            }
        }
        
        async function fetchStats() {
            if (API_BASE) {
                try {
                    const response = await fetch(`${API_BASE}/api/stats`);
                    if (response.ok) {
                        const stats = await response.json();
                        updateDashboard(stats);
                        return;
                    }
                } catch (error) {
                    console.log('API not available, trying static stats');
                }
            }
            
            for (const path of STATS_PATHS) {
                try {
                    const response = await fetch(path);
                    if (response.ok) {
                        const stats = await response.json();
                        statsData = stats;
                        updateDashboard(stats);
                        return;
                    }
                } catch (error) {
                    continue;
                }
            }
            
            console.error('Failed to fetch stats from all sources');
            showError('Unable to load collection statistics. Stats may be updating...');
            updateDashboard({
                totalImages: 0,
                activeCameras: 82,
                todayImages: 0,
                latestCapture: null,
                cameraCounts: {},
                storageInfo: 'Stats unavailable',
                collectionPeriod: { days: 0, start: null, end: null },
            });
        }
        
        function updateDashboard(stats) {
            document.getElementById('totalImages').textContent = (stats.totalImages || 0).toLocaleString();
            document.getElementById('activeCameras').textContent = stats.activeCameras || 82;
            document.getElementById('todayImages').textContent = (stats.todayImages || 0).toLocaleString();
            
            if (stats.latestCapture) {
                const captureDate = new Date(stats.latestCapture);
                document.getElementById('latestCapture').textContent = captureDate.toLocaleString();
                document.getElementById('latestCaptureSubtext').textContent = 
                    stats.latestCameraName || 'Latest collection';
            } else {
                document.getElementById('latestCapture').textContent = '--';
            }
            
            if (stats.collectionPeriod) {
                document.getElementById('collectionDays').textContent = stats.collectionPeriod.days || 0;
                if (stats.collectionPeriod.start && stats.collectionPeriod.end) {
                    document.getElementById('collectionPeriod').textContent = 
                        `${stats.collectionPeriod.start} to ${stats.collectionPeriod.end}`;
                }
            }
            
            // Fetch and display latest image
            fetchLatestImage(stats);
            
            // Update camera grid
            const cameraGrid = document.getElementById('cameraGrid');
            cameraGrid.innerHTML = '';
            
            if (cameraManifest.length > 0) {
                const sortedCameras = [...cameraManifest].sort((a, b) => 
                    (a.number || 999) - (b.number || 999)
                );
                
                sortedCameras.forEach(camera => {
                    const cameraData = stats.cameraCounts?.[camera.id] || { count: 0, lastCapture: null };
                    const card = createCameraCard(camera, cameraData);
                    cameraGrid.appendChild(card);
                });
            } else if (stats.cameraCounts) {
                Object.entries(stats.cameraCounts).forEach(([cameraId, data]) => {
                    const camera = { id: cameraId, name: cameraId, number: null };
                    const card = createCameraCard(camera, data);
                    cameraGrid.appendChild(card);
                });
            }
        }
        
        function createCameraCard(camera, data) {
            const card = document.createElement('div');
            card.className = 'camera-card';
            
            const lastCapture = data.lastCapture 
                ? new Date(data.lastCapture).toLocaleString()
                : 'No captures yet';
            
            const cameraNumber = camera.number ? `#${camera.number}` : '';
            const cameraName = camera.name || camera.id;
            
            card.innerHTML = `
                <div class="camera-header">
                    <div class="camera-name">${cameraNumber ? cameraNumber + ': ' : ''}${cameraName}</div>
                    ${camera.number ? `<div class="camera-badge">#${camera.number}</div>` : ''}
                </div>
                ${camera.latitude ? `<div class="camera-meta">${camera.latitude.toFixed(4)}, ${camera.longitude.toFixed(4)}</div>` : ''}
                <div class="camera-count">${(data.count || 0).toLocaleString()}</div>
                <div class="camera-last-capture">Last: ${lastCapture}</div>
            `;
            
            return card;
        }
        
        function showError(message) {
            const errorDiv = document.getElementById('errorMessage');
            errorDiv.textContent = message;
            errorDiv.style.display = 'block';
            setTimeout(() => {
                errorDiv.style.display = 'none';
            }, 5000);
        }
        
        async function initialize() {
            await loadManifest();
            await fetchStats();
        }
        
        initialize();
        
        setInterval(fetchStats, 30000);
    </script>
</body>
</html>
